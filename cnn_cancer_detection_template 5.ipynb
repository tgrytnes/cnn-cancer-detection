{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f83f91",
   "metadata": {},
   "source": [
    "# CNN Cancer Detection â€“ Weekâ€‘3 Notebook\n",
    "Kaggle Histopathologic Cancer Detection challenge\n",
    "\n",
    "*Goal:* build & iterate CNN models to classify histology image tiles as cancer vs. normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928844a2-53b8-41e3-ae5b-f94448f2bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Environment & secrets (run once per kernel / after pod restart)\n",
    "\n",
    "# Install & non-TF imports\n",
    "%pip install -q kaggle wandb pandas matplotlib pillow tensorflow scikit-learn\n",
    "import os, random, pathlib, numpy as np, wandb\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import hashlib, time\n",
    "from pathlib import Path\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# TensorFlow imports & GPU setup\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth before any TF GPU work\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Enable mixed precision\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Now import Keras modules\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, metrics\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559697c-c0ad-4baf-a076-60f8fd7c7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure persistent SSH dir exists\n",
    "mkdir -p /workspace/.ssh\n",
    "\n",
    "# Copy into rootâ€™s home with strict perms\n",
    "rm -rf /root/.ssh\n",
    "mkdir -p /root/.ssh\n",
    "cp -r /workspace/.ssh/* /root/.ssh/\n",
    "chmod 700 /root/.ssh\n",
    "chmod 600 /root/.ssh/id_ed25519 /root/.ssh/config\n",
    "chmod 644 /root/.ssh/id_ed25519.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a9494b-5338-4210-9154-12cbba04e709",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/workspace/kaggle.json': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE_CONFIG_DIR â†’ /workspace/.kaggle\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /workspace/.kaggle/kaggle.json'\n",
      "Configuration values from /workspace/.kaggle\n",
      "- username: thomasfeygrytnes\n",
      "- path: None\n",
      "- proxy: None\n",
      "- competition: None\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1) Create the config directory under /workspace\n",
    "mkdir -p /workspace/.kaggle\n",
    "chmod 700 /workspace/.kaggle\n",
    "\n",
    "# 2) Move  uploaded kaggle.json into it\n",
    "mv /workspace/kaggle.json /workspace/.kaggle/kaggle.json\n",
    "\n",
    "# 3) Lock down permissions\n",
    "chmod 600 /workspace/.kaggle/kaggle.json\n",
    "\n",
    "# 4) Export the env variable so the Kaggle CLI picks it up\n",
    "export KAGGLE_CONFIG_DIR=/workspace/.kaggle\n",
    "\n",
    "# 5) Verify\n",
    "echo \"KAGGLE_CONFIG_DIR â†’ $KAGGLE_CONFIG_DIR\"\n",
    "kaggle config view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0b73c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "setup",
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-m8xa7mf\u001b[0m (\u001b[33mthomas-m8xa7mf-student-beans\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€ Reproducibility -----------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED);\n",
    "\n",
    "# â”€â”€ Paths ---------------------------------------------------------------\n",
    "PROJECT_ROOT = pathlib.Path.cwd()                  # usually /workspace\n",
    "DATA_DIR      = pathlib.Path(\"/workspace/data\")    # persistent dataset\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# â”€â”€ Kaggle CLI: tell it where the token lives ---------------------------\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/workspace/.kaggle\"\n",
    "\n",
    "# â”€â”€ W&B: persistent, prompt-once login ----------------------------------\n",
    "KEY_FILE = pathlib.Path(\"/workspace/.wandb_api_key\")\n",
    "if KEY_FILE.exists():                         # reuse stored key\n",
    "    os.environ[\"WANDB_API_KEY\"] = KEY_FILE.read_text().strip()\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True)\n",
    "else:                                         # first run â†’ ask & cache\n",
    "    wandb.login()                             # paste key when prompted\n",
    "    saved_key = os.getenv(\"WANDB_API_KEY\")\n",
    "    if saved_key:\n",
    "        KEY_FILE.write_text(saved_key)\n",
    "        KEY_FILE.chmod(0o600)                 # read/write for you only\n",
    "        print(f\"W&B API key saved to {KEY_FILE}\")\n",
    "# -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a5bba",
   "metadata": {},
   "source": [
    "## Data download & verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]             \n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1270 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3207 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1918 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5103 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1575 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5290 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3518 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
      "Fetched 42.5 MB in 2s (17.5 MB/s)                             \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following packages will be upgraded:\n",
      "  unzip\n",
      "1 upgraded, 0 newly installed, 0 to remove and 145 not upgraded.\n",
      "Need to get 175 kB of archives.\n",
      "After this operation, 1024 B of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Fetched 175 kB in 0s (2094 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 20752 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "Unpacking unzip (6.0-26ubuntu3.2) over (6.0-26ubuntu3.1) ...\n",
      "Setting up unzip (6.0-26ubuntu3.2) ...\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /workspace/.kaggle/kaggle.json'\n",
      "Downloading histopathologic-cancer-detection.zip to data\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 6.28G/6.31G [00:27<00:00, 338MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.31G/6.31G [00:27<00:00, 247MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Only run first time on new server!!\n",
    "\n",
    "!apt-get update && apt-get install -y unzip\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"/workspace/data\")   # persistent volume\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "# Download competition data (~1.2â€¯GB) to ./data\n",
    "!kaggle competitions download -c histopathologic-cancer-detection -p data\n",
    "!unzip -q data/histopathologic-cancer-detection.zip -d data\n",
    "import glob, json, subprocess, pathlib\n",
    "print(len(glob.glob('data/train/*.tif')), 'training tiles')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb90184-fcd2-4dc3-b401-413048f26566",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"/workspace/data\")\n",
    "df       = pd.read_csv(DATA_DIR / \"train_labels.csv\")\n",
    "all_paths = [DATA_DIR/\"train\"/f\"{img_id}.tif\" for img_id in df.id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61227b5e",
   "metadata": {},
   "source": [
    "# Data Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff5a246-c0c9-4d42-a8c8-3ebaf44c7422",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying files:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 121653/220025 [17:06<17:48, 92.06it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us check if we have any broken files\n",
    "\n",
    "broken = []\n",
    "for p in tqdm(all_paths, desc=\"Verifying files\"):\n",
    "    try:\n",
    "        img = Image.open(p)\n",
    "        img.verify()\n",
    "    except Exception:\n",
    "        broken.append(p)\n",
    "\n",
    "print(f\"Unreadable/corrupted files: {len(broken)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cce92b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking zero-byte files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220025/220025 [01:29<00:00, 2446.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 zero-byte files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us check if we have any files with zero bytes\n",
    "\n",
    "zero_byte = []\n",
    "for p in tqdm(all_paths, desc=\"Checking zero-byte files\"):\n",
    "    if os.path.getsize(p) == 0:\n",
    "        zero_byte.append(p)\n",
    "\n",
    "print(f\"Found {len(zero_byte)} zero-byte files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6463022e-c7d5-4da6-b61d-2f384cf2544f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking duplicates:   3%|â–Ž         | 7360/220025 [00:55<26:57, 131.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m hashes, dupes = {}, []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m tqdm(all_paths, desc=\u001b[33m\"\u001b[39m\u001b[33mChecking duplicates\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     h = hashlib.md5(\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m).hexdigest()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hashes: dupes.append((hashes[h], p))\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:            hashes[h] = p\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1051\u001b[39m, in \u001b[36mPath.read_bytes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mOpen the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.open(mode=\u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Let us check if we have any duplicate files\n",
    "\n",
    "hashes, dupes = {}, []\n",
    "for p in tqdm(all_paths, desc=\"Checking duplicates\"):\n",
    "    h = hashlib.md5(p.read_bytes()).hexdigest()\n",
    "    if h in hashes: dupes.append((hashes[h], p))\n",
    "    else:            hashes[h] = p\n",
    "print(f\"Found {len(dupes)} duplicate pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f17c4f-c553-4464-9de3-a73288c0d17f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking structure:   1%|          | 2286/220025 [00:09<14:55, 243.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m structural_issues = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m tqdm(all_paths, desc=\u001b[33m\"\u001b[39m\u001b[33mChecking structure\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m# #dimensions: expect 96Ã—96\u001b[39;00m\n\u001b[32m      8\u001b[39m         size_ok = img.size == (\u001b[32m96\u001b[39m, \u001b[32m96\u001b[39m)\n\u001b[32m      9\u001b[39m         \u001b[38;5;66;03m# #colormode: expect RGB\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3466\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3464\u001b[39m filename: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mbytes\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m-> \u001b[39m\u001b[32m3466\u001b[39m     filename = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m   3469\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:416\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:451\u001b[39m, in \u001b[36m_joinrealpath\u001b[39m\u001b[34m(path, rest, strict, seen)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Let us check that all the structural properties of the files are okay.\n",
    "\n",
    "structural_issues = []\n",
    "\n",
    "for p in tqdm(all_paths, desc=\"Checking structure\"):\n",
    "    with Image.open(p) as img:\n",
    "        # #dimensions: expect 96Ã—96\n",
    "        size_ok = img.size == (96, 96)\n",
    "        # #colormode: expect RGB\n",
    "        mode_ok = img.mode == \"RGB\"\n",
    "        # #datatype: expect uint8 pixels\n",
    "        arr = np.array(img)\n",
    "        dtype_ok = arr.dtype == np.uint8\n",
    "\n",
    "        if not (size_ok and mode_ok and dtype_ok):\n",
    "            structural_issues.append((p, img.size, img.mode, arr.dtype))\n",
    "\n",
    "print(f\"Found {len(structural_issues)} files with unexpected structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c324cb-52af-4b8d-a9ee-aa6a6ce1b154",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2ad411-d4f8-4990-9baa-39e819e83618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes\u001b[38;5;241m.\u001b[39mravel(), sample):\n\u001b[0;32m---> 11\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mDATA_DIR\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(df[df\u001b[38;5;241m.\u001b[39mid\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mimg_id]\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class balance\n",
    "df.label.value_counts().sort_index().plot.bar(rot=0, width=0.6)\n",
    "plt.title(\"Class balance (0 = benign, 1 = tumor)\")\n",
    "plt.ylabel(\"Tile count\"); plt.xlabel(\"Label\")\n",
    "plt.show()\n",
    "\n",
    "# 3Ã—3 sample grid\n",
    "sample = random.sample(df.id.tolist(), 9)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "for ax, img_id in zip(axes.ravel(), sample):\n",
    "    ax.imshow(Image.open(DATA_DIR / \"train\" / f\"{img_id}.tif\"))\n",
    "    ax.set_title(f\"Label: {int(df[df.id == img_id].label.values[0])}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0f4f9-a9f1-4cb4-9d59-0f1882fb1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-channel mean & std (1000-sample)\n",
    "sample = random.sample(all_paths, 1000)\n",
    "arr = np.stack([np.array(Image.open(p)) for p in sample])\n",
    "print(\"Means:\", arr.mean((0,1,2)), \"Stds:\", arr.std((0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa478c0-b7d1-4e22-a592-e8fef2a6a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB histograms stratified by label (500/sample)\n",
    "df['path'] = df['id'].apply(lambda i: DATA_DIR/'train'/f\"{i}.tif\")\n",
    "for l in [0,1]:\n",
    "    v = np.array([np.array(Image.open(p)).ravel()\n",
    "                  for p in random.sample(df[df.label==l].path.tolist(), 500)]).ravel()\n",
    "    plt.hist(v, bins=50, alpha=0.5, label=f\"Label {l}\")\n",
    "plt.legend(); plt.title(\"Pixelâ€value distributions\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45444b86-69d4-41c4-b149-dcab69e27c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brightnessâ€based outliers (1st/99th percentiles)\n",
    "b = [(p, np.array(Image.open(p)).mean()) for p in random.sample(all_paths, 5000)]\n",
    "lo, hi = np.percentile([val for _, val in b], [1, 99])\n",
    "outliers = [p for p, val in b if val < lo or val > hi]\n",
    "print(\"Brightness outliers:\", len(outliers))\n",
    "\n",
    "\n",
    "# Sort by mean brightness\n",
    "b_sorted = sorted(b, key=lambda x: x[1])\n",
    "darkest, brightest = b_sorted[:5], b_sorted[-5:]\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for ax, (p, v) in zip(axs.ravel(), darkest + brightest):\n",
    "    ax.imshow(Image.open(p)); ax.set_title(f\"{v:.1f}\"); ax.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c14f80-0f9b-4da3-a2f8-ff45e71fc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on flattened pixels (1000-sample â†’ 2D)\n",
    "from sklearn.decomposition import PCA\n",
    "sample = random.sample(all_paths, 1000)\n",
    "data = np.array([np.array(Image.open(p)).flatten() for p in sample])\n",
    "print(\"PCA shape:\", PCA(2).fit_transform(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa684e1-a018-4b0f-be1f-48f9ec723e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1 000 patches\n",
    "sample_paths = random.sample(all_paths, 1000)\n",
    "\n",
    "# Build a (1000, 96Ã—96Ã—3) â†’ (1000, 27648) data matrix\n",
    "data = np.stack([np.array(Image.open(p)).flatten() for p in sample_paths])\n",
    "\n",
    "# Fit PCA to 2 components and transform\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(data)\n",
    "\n",
    "# Grab labels\n",
    "labels = [ df.loc[df.id == p.stem, 'label'].item() for p in sample_paths ]\n",
    "\n",
    "# Scatter-plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(coords[:, 0], coords[:, 1], c=labels, alpha=0.6, s=20)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"2D PCA of 1 000 Image Patches\")\n",
    "plt.show()\n",
    "\n",
    "# How much variance do these axes explain?\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e1e8d",
   "metadata": {},
   "source": [
    "# Preâ€‘processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4b08f-9ce1-4a8b-97df-f1ee52cb0a13",
   "metadata": {},
   "source": [
    "## Preprocessing: TIFF â†’ JPEG + TFRecord sharding + Resize & Standardization\n",
    "\n",
    "We do three main steps before training to make data loading fast and smooth for **every** model:\n",
    "\n",
    "1. **Convert TIFF â†’ JPEG**  \n",
    "   - **What:** Read each `.tif` image, convert to RGB, save as a `.jpg`  \n",
    "   - **Why:**  \n",
    "     - JPEG decoding is **hardware-accelerated** on GPU/TPU  \n",
    "     - Files are much smaller â†’ faster disk I/O  \n",
    "     - Removes slow, Python-level TIFF parsing\n",
    "\n",
    "2. **Shard into TFRecord files**  \n",
    "   - **What:**  \n",
    "     - Read each JPEG byte string + its label  \n",
    "     - Pack them into `tf.train.Example` protobufs  \n",
    "     - Write ~10 `.tfrecord` shards for **train** and ~10 for **val**  \n",
    "   - **Why:**  \n",
    "     - Avoids opening thousands of small files one by one  \n",
    "     - `tf.data` can **interleave** shard reads in parallel  \n",
    "     - Prefetching and batching become much more efficient\n",
    "\n",
    "3. **Resize & Standardize**  \n",
    "   - **What:**  \n",
    "     - Decode JPEG â†’ tensor, then `tf.image.resize` to your modelâ€™s input size (e.g. 224Ã—224)  \n",
    "     - Scale pixels to `[0,1]`: `img = tf.cast(img, tf.float32) / 255.0`  \n",
    "     - Standardize with ImageNet mean/std:  \n",
    "       ```python\n",
    "       mean = [0.485, 0.456, 0.406]\n",
    "       std  = [0.229, 0.224, 0.225]\n",
    "       img  = (img - mean) / std\n",
    "       ```  \n",
    "   - **Why:**  \n",
    "     - Ensures every model sees the same input shape  \n",
    "     - Matches the normalization expected by pretrained backbones  \n",
    "     - Helps models converge faster and more stably\n",
    "\n",
    "---\n",
    "\n",
    "### After preprocessing we have:\n",
    "\n",
    "- **`data/train_jpg/*.jpg`** â€” all images in fast-loading JPEG format  \n",
    "- **`data/train_tfr/shard-*.tfrecord`** â€” sharded TFRecord files for training  \n",
    "- **`data/val_tfr/shard-*.tfrecord`** â€” sharded TFRecord files for validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec9ccbb-8d50-4993-af64-9aca1d2e0293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting TIFFâ†’JPEG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220025/220025 [00:15<00:00, 14618.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert all .tif â†’ .jpg\n",
    "src_dir = Path(\"data/train\")\n",
    "dst_dir = Path(\"data/train_jpg\")\n",
    "dst_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def convert_tif_to_jpg(tif_path: Path):\n",
    "    jpg_path = dst_dir / tif_path.with_suffix(\".jpg\").name\n",
    "    if not jpg_path.exists():\n",
    "        img = Image.open(tif_path).convert(\"RGB\")\n",
    "        img.save(jpg_path, \"JPEG\", quality=90)\n",
    "\n",
    "tif_paths = list(src_dir.glob(\"*.tif\"))\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    list(tqdm(executor.map(convert_tif_to_jpg, tif_paths),\n",
    "              total=len(tif_paths),\n",
    "              desc=\"Converting TIFFâ†’JPEG\"))\n",
    "\n",
    "# Load labels & point to JPEGs\n",
    "df = pd.read_csv(\"data/train_labels.csv\")\n",
    "df[\"filename\"] = df.id.astype(str) + \".jpg\"\n",
    "df[\"label\"]    = df.label.astype(np.float32)\n",
    "df[\"filepath\"] = df[\"filename\"].apply(lambda fn: str(dst_dir / fn))\n",
    "\n",
    "# Stratified train/validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# TFRecord helper functions\n",
    "def _bytes_feature(x):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[x]))\n",
    "\n",
    "def _float_feature(x):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[x]))\n",
    "\n",
    "def write_tfrecord(shard_idx, samples, out_dir):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tfpath = out_dir / f\"shard-{shard_idx:03d}.tfrecord\"\n",
    "    with tf.io.TFRecordWriter(str(tfpath)) as writer:\n",
    "        for filepath, label in samples:\n",
    "            img_bytes = tf.io.read_file(filepath)\n",
    "            feat = {\n",
    "                \"image\": _bytes_feature(img_bytes.numpy()),\n",
    "                \"label\": _float_feature(float(label))\n",
    "            }\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feat))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "# Shard and write TFRecords for both train and val\n",
    "for split_name, split_df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "    records = list(zip(split_df[\"filepath\"].values, split_df[\"label\"].values))\n",
    "    shards = np.array_split(records, 10)\n",
    "    for i, shard in enumerate(shards):\n",
    "        write_tfrecord(i, shard, out_dir=f\"data/{split_name}_tfr\")\n",
    "\n",
    "# Build tf.data pipelines with resize & ImageNetâ€style norm\n",
    "IMG_SIZE   = (96, 96)\n",
    "BATCH_SIZE = 64\n",
    "SEED       = 42\n",
    "\n",
    "# ImageNet mean/std\n",
    "mean = tf.constant([0.485, 0.456, 0.406], shape=[1,1,3], dtype=tf.float32)\n",
    "std  = tf.constant([0.229, 0.224, 0.225], shape=[1,1,3], dtype=tf.float32)\n",
    "\n",
    "feature_desc = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_and_norm(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    img = tf.image.decode_jpeg(parsed[\"image\"], channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    img = (img - mean) / std\n",
    "    return img, parsed[\"label\"]\n",
    "\n",
    "def make_tfr_dataset(pattern, training):\n",
    "    ds = tf.data.Dataset.list_files(pattern, seed=SEED)\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.map(_parse_and_norm, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(10000, seed=SEED)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_tfr_dataset(\"data/train_tfr/*.tfrecord\", training=True)\n",
    "val_ds   = make_tfr_dataset(\"data/val_tfr/*.tfrecord\",   training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b8a02-6c50-49a8-91d4-483ace45fa22",
   "metadata": {},
   "source": [
    "# Define and Train CNN Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a19b54-4544-4341-880e-ccfa10269fa5",
   "metadata": {},
   "source": [
    "### Super-Simple One-Block CNN\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "1. **Input** â€“ 96 Ã— 96 RGB patch  \n",
    "2. **Conv2D(16, 3Ã—3, ReLU)** â€“ learns 16 small filters; `padding=\"same\"` keeps 96Ã—96 size  \n",
    "3. **MaxPooling2D(2Ã—2)** â€“ downsamples to 48Ã—48Ã—16, adding translation invariance  \n",
    "4. **Flatten** â€“ turns the 48Ã—48Ã—16 feature map into a 36 864-dimensional vector  \n",
    "5. **Dense(1, Sigmoid)** â€“ outputs the probability (e.g., â€œmalignantâ€); train with binary cross-entropy  \n",
    "\n",
    "**Why Start Here?**\n",
    "\n",
    "* ~6 K parameters â†’ trains in seconds  \n",
    "* Demonstrates core CNN ideas: locality (3Ã—3 filters) and invariance (pooling)  \n",
    "* Provides a simple baseline for comparisonâ€”larger or pretrained models should outperform  \n",
    "* Easy to debugâ€”if this fails, check the data and training loop before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e2c02e-8ad7-4bff-a532-dad03a4a03ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 96, 96, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 48, 48, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 36864)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 36865     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37313 (145.75 KB)\n",
      "Trainable params: 37313 (145.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "   6/2751 [..............................] - ETA: 4:15 - loss: 2.8261 - accuracy: 0.4870 - auc: 0.4851WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0407s vs `on_train_batch_end` time: 0.0481s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0407s vs `on_train_batch_end` time: 0.0481s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2751/2751 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.7879 - auc: 0.8541\n",
      "Epoch 1: val_auc improved from -inf to 0.89091, saving model to best_simple.h5\n",
      "2751/2751 [==============================] - 407s 147ms/step - loss: 0.4730 - accuracy: 0.7879 - auc: 0.8541 - val_loss: 0.4085 - val_accuracy: 0.8181 - val_auc: 0.8909\n",
      "Epoch 2/5\n",
      "2745/2751 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8234 - auc: 0.8936\n",
      "Epoch 2: val_auc improved from 0.89091 to 0.89938, saving model to best_simple.h5\n",
      "2751/2751 [==============================] - 428s 156ms/step - loss: 0.4019 - accuracy: 0.8234 - auc: 0.8936 - val_loss: 0.3893 - val_accuracy: 0.8286 - val_auc: 0.8994\n",
      "Epoch 3/5\n",
      "2748/2751 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8357 - auc: 0.9070\n",
      "Epoch 3: val_auc improved from 0.89938 to 0.90500, saving model to best_simple.h5\n",
      "2751/2751 [==============================] - 436s 158ms/step - loss: 0.3773 - accuracy: 0.8357 - auc: 0.9070 - val_loss: 0.4039 - val_accuracy: 0.8297 - val_auc: 0.9050\n",
      "Epoch 4/5\n",
      " 715/2751 [======>.......................] - ETA: 4:22 - loss: 0.3472 - accuracy: 0.8499 - auc: 0.9222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2749/2751 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8445 - auc: 0.9168\n",
      "Epoch 4: val_auc did not improve from 0.90500\n",
      "2751/2751 [==============================] - 435s 158ms/step - loss: 0.3580 - accuracy: 0.8445 - auc: 0.9168 - val_loss: 0.4237 - val_accuracy: 0.8147 - val_auc: 0.8904\n",
      "Epoch 5/5\n",
      "2748/2751 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8519 - auc: 0.9242\n",
      "Epoch 5: val_auc did not improve from 0.90500\n",
      "2751/2751 [==============================] - 434s 158ms/step - loss: 0.3432 - accuracy: 0.8519 - auc: 0.9242 - val_loss: 0.4389 - val_accuracy: 0.8133 - val_auc: 0.9008\n",
      "Simple CNN final metrics: {'loss': 0.3432409465312958, 'accuracy': 0.851891815662384, 'auc': 0.924227237701416, 'val_loss': 0.43889549374580383, 'val_accuracy': 0.8132712244987488, 'val_auc': 0.9008442163467407}\n"
     ]
    }
   ],
   "source": [
    "# Define a superâ€simple oneâ€block CNN\n",
    "\n",
    "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "\n",
    "# One convolutional block\n",
    "x = layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "# Classifier head\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_simple = models.Model(inputs, outputs)\n",
    "model_simple.summary()\n",
    "\n",
    "# Compile\n",
    "model_simple.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_simple = ModelCheckpoint(\n",
    "    \"best_simple.h5\",\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train for a few epochs\n",
    "history_simple = model_simple.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[WandbCallback(), checkpoint_simple],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Report final metrics\n",
    "final_simple = {k: history_simple.history[k][-1] for k in history_simple.history}\n",
    "print(\"Simple CNN final metrics:\", final_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2b616-f91a-4be7-b331-a85db4a67d6e",
   "metadata": {},
   "source": [
    "**Final Metrics**  \n",
    "- **Training:** accuracy 85.2%, AUC 0.924  \n",
    "- **Validation:** accuracy 81.3%, AUC 0.901  \n",
    "\n",
    "**Key Takeaways**  \n",
    "- AUC > 0.90 on both sets shows strong discrimination.  \n",
    "- Small trainâ€“val gap (â‰ˆ4% acc, 0.02 AUC) indicates limited over-fitting.  \n",
    "- Impressively strong for a ~6 K-parameter model.\n",
    "\n",
    "**Recommended Next Steps**  \n",
    "1. **Data augmentation** (e.g. rotations, flips)  \n",
    "2. **Regularization** (dropout, L2 weight decay)  \n",
    "3. **Deeper CNN** (add extra Convâ†’Pool blocks)  \n",
    "4. **Transfer learning** (fine-tune a lightweight pretrained backbone)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d396a8-1c7a-4683-9c54-a4fd603d5fc2",
   "metadata": {},
   "source": [
    "## Why Try a Pretrained Model?\n",
    "\n",
    "After training our simple custom CNN, we now evaluate a more powerful alternative: a pretrained model (MobileNetV2). This approach makes sense for several reasons:\n",
    "\n",
    "- **Transfer learning**: MobileNetV2 was trained on ImageNet, which gives it rich low-level features (edges, textures, shapes) that often transfer well to medical image tasks.\n",
    "- **Stronger baseline**: Comparing results helps us understand how much value transfer learning adds over a small, scratch-trained model.\n",
    "- **Faster convergence**: Pretrained models usually require fewer epochs and less data to achieve good performance.\n",
    "- **Real-world relevance**: In practice, pretrained architectures are widely used in computer vision pipelines â€” it's essential to know how to fine-tune and evaluate them.\n",
    "\n",
    "This experiment helps benchmark our custom model against a well-established CNN backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22407125-3fa9-4730-b340-961519ab358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:00:24.236960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43710 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:56:00.0, compute capability: 8.6\n",
      "2025-08-07 13:00:24.381874: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:00:30.595347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2025-08-07 13:00:30.670190: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-07 13:00:33.025142: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x144dac60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-07 13:00:33.025180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2025-08-07 13:00:33.032242: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-07 13:00:33.144123: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376/1376 [==============================] - 41s 25ms/step - loss: 0.4630 - accuracy: 0.7879 - auc: 0.8581 - val_loss: 0.3633 - val_accuracy: 0.8410 - val_auc: 0.9131 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "1376/1376 [==============================] - 33s 24ms/step - loss: 0.3784 - accuracy: 0.8348 - auc: 0.9057 - val_loss: 0.3378 - val_accuracy: 0.8556 - val_auc: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8461 - auc: 0.9160\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "1376/1376 [==============================] - 34s 24ms/step - loss: 0.3568 - accuracy: 0.8461 - auc: 0.9160 - val_loss: 0.3277 - val_accuracy: 0.8605 - val_auc: 0.9299 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "1376/1376 [==============================] - 35s 24ms/step - loss: 0.3489 - accuracy: 0.8493 - auc: 0.9198 - val_loss: 0.3244 - val_accuracy: 0.8619 - val_auc: 0.9314 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8514 - auc: 0.9213\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "1376/1376 [==============================] - 31s 22ms/step - loss: 0.3456 - accuracy: 0.8514 - auc: 0.9214 - val_loss: 0.3218 - val_accuracy: 0.8627 - val_auc: 0.9326 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "1376/1376 [==============================] - 34s 24ms/step - loss: 0.3434 - accuracy: 0.8524 - auc: 0.9223 - val_loss: 0.3207 - val_accuracy: 0.8631 - val_auc: 0.9330 - lr: 2.5000e-05\n",
      "Epoch 7/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3417 - accuracy: 0.8543 - auc: 0.9231\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "1376/1376 [==============================] - 34s 24ms/step - loss: 0.3417 - accuracy: 0.8543 - auc: 0.9231 - val_loss: 0.3202 - val_accuracy: 0.8642 - val_auc: 0.9334 - lr: 2.5000e-05\n",
      "Epoch 8/10\n",
      "1376/1376 [==============================] - 35s 25ms/step - loss: 0.3424 - accuracy: 0.8531 - auc: 0.9228 - val_loss: 0.3193 - val_accuracy: 0.8637 - val_auc: 0.9336 - lr: 1.2500e-05\n",
      "Epoch 9/10\n",
      "1376/1376 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.8537 - auc: 0.9235\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "1376/1376 [==============================] - 36s 25ms/step - loss: 0.3407 - accuracy: 0.8537 - auc: 0.9235 - val_loss: 0.3190 - val_accuracy: 0.8639 - val_auc: 0.9338 - lr: 1.2500e-05\n",
      "Epoch 10/10\n",
      "1376/1376 [==============================] - 33s 23ms/step - loss: 0.3402 - accuracy: 0.8543 - auc: 0.9238 - val_loss: 0.3188 - val_accuracy: 0.8642 - val_auc: 0.9339 - lr: 6.2500e-06\n"
     ]
    }
   ],
   "source": [
    "# Setting up a a pretrained model: MobileNetV2\n",
    "\n",
    "IMG_SIZE   = (96, 96)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS     = 10\n",
    "SEED       = 42\n",
    "\n",
    "feature_desc = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_example(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    img = tf.image.decode_jpeg(parsed[\"image\"], channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = preprocess_input(img)  # required for MobileNetV2\n",
    "    return img, parsed[\"label\"]\n",
    "\n",
    "def make_tfr_dataset(pattern, training):\n",
    "    ds = tf.data.Dataset.list_files(pattern, seed=SEED)\n",
    "    ds = ds.interleave(\n",
    "        lambda fn: tf.data.TFRecordDataset(fn),\n",
    "        cycle_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.map(_parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(10_000, seed=SEED)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_tfr_dataset(\"data/train_tfr/*.tfrecord\", training=True)\n",
    "val_ds   = make_tfr_dataset(\"data/val_tfr/*.tfrecord\",   training=False)\n",
    "\n",
    "base = MobileNetV2(input_shape=(*IMG_SIZE, 3), include_top=False, weights=\"imagenet\")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n",
    "\n",
    "model_mobilenet = models.Model(inputs, outputs)\n",
    "model_mobilenet.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "cbs = [\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_mobilenet = model_mobilenet.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb30446-7dc7-487e-937a-f3ef139daef3",
   "metadata": {},
   "source": [
    "### Summary and conlusion for using a pretrained model: MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f04a43-1a49-4f6e-8cbb-16b43f6b6cd6",
   "metadata": {},
   "source": [
    "## Why Try EfficientNet-B4?\n",
    "\n",
    "After evaluating MobileNetV2, we now explore a more powerful pretrained architecture: **EfficientNet-B4**. This step is motivated by the following reasons:\n",
    "\n",
    "- **Stronger feature extraction**: EfficientNet-B4 has significantly more capacity than MobileNetV2. It scales depth, width, and input resolution in a principled way, leading to improved performance on many image tasks.\n",
    "- **Higher input resolution (224Ã—224)**: This allows the model to capture more spatial detail from the histopathology images, which is important for detecting small cancer regions.\n",
    "- **Benchmark progression**: Testing a progressively stronger model helps us understand the trade-offs between model complexity and performance.\n",
    "- **Pretrained on ImageNet**: Like MobileNet, EfficientNet-B4 benefits from transfer learning, but with better scaling and optimization strategies.\n",
    "\n",
    "This experiment helps us assess whether upgrading the backbone yields meaningful gains in AUC and overall model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad5138e-db66-4cc5-91d1-ac3d61a5c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "71686520/71686520 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "1376/1376 [==============================] - 238s 163ms/step - loss: 0.4686 - accuracy: 0.7882 - auc: 0.8595 - val_loss: 0.3911 - val_accuracy: 0.8310 - val_auc: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "1376/1376 [==============================] - 227s 161ms/step - loss: 0.3838 - accuracy: 0.8320 - auc: 0.9042 - val_loss: 0.3608 - val_accuracy: 0.8439 - val_auc: 0.9167 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8405 - auc: 0.9126\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "1376/1376 [==============================] - 228s 161ms/step - loss: 0.3651 - accuracy: 0.8405 - auc: 0.9126 - val_loss: 0.3469 - val_accuracy: 0.8492 - val_auc: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "1376/1376 [==============================] - 214s 151ms/step - loss: 0.3576 - accuracy: 0.8438 - auc: 0.9159 - val_loss: 0.3422 - val_accuracy: 0.8518 - val_auc: 0.9244 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8458 - auc: 0.9176\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "1376/1376 [==============================] - 214s 154ms/step - loss: 0.3540 - accuracy: 0.8458 - auc: 0.9176 - val_loss: 0.3384 - val_accuracy: 0.8540 - val_auc: 0.9260 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "1376/1376 [==============================] - 218s 157ms/step - loss: 0.3517 - accuracy: 0.8470 - auc: 0.9184 - val_loss: 0.3367 - val_accuracy: 0.8545 - val_auc: 0.9266 - lr: 2.5000e-05\n",
      "Epoch 7/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8471 - auc: 0.9190\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "1376/1376 [==============================] - 224s 162ms/step - loss: 0.3504 - accuracy: 0.8471 - auc: 0.9190 - val_loss: 0.3352 - val_accuracy: 0.8552 - val_auc: 0.9274 - lr: 2.5000e-05\n",
      "Epoch 8/10\n",
      "1376/1376 [==============================] - 229s 162ms/step - loss: 0.3490 - accuracy: 0.8483 - auc: 0.9197 - val_loss: 0.3344 - val_accuracy: 0.8558 - val_auc: 0.9277 - lr: 1.2500e-05\n",
      "Epoch 9/10\n",
      "1375/1376 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8490 - auc: 0.9198\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "1376/1376 [==============================] - 230s 163ms/step - loss: 0.3484 - accuracy: 0.8490 - auc: 0.9198 - val_loss: 0.3338 - val_accuracy: 0.8558 - val_auc: 0.9279 - lr: 1.2500e-05\n",
      "Epoch 10/10\n",
      "1376/1376 [==============================] - 225s 159ms/step - loss: 0.3475 - accuracy: 0.8495 - auc: 0.9203 - val_loss: 0.3334 - val_accuracy: 0.8561 - val_auc: 0.9280 - lr: 6.2500e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS     = 10\n",
    "SEED       = 42\n",
    "\n",
    "feature_desc = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_example(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    img = tf.image.decode_jpeg(parsed[\"image\"], channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = efficientnet_preprocess(img)\n",
    "    return img, parsed[\"label\"]\n",
    "\n",
    "def make_tfr_dataset(pattern, training):\n",
    "    ds = tf.data.Dataset.list_files(pattern, seed=SEED)\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.map(_parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(10_000, seed=SEED)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_tfr_dataset(\"data/train_tfr/*.tfrecord\", training=True)\n",
    "val_ds   = make_tfr_dataset(\"data/val_tfr/*.tfrecord\",   training=False)\n",
    "\n",
    "base = EfficientNetB4(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n",
    "\n",
    "model_efficientnet = models.Model(inputs, outputs)\n",
    "model_efficientnet.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "cbs = [\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_efficientnet = model_efficientnet.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f307565-5669-4d0e-993b-4b31c1719bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514668f2-d853-4713-b03e-474fbe1521fb",
   "metadata": {},
   "source": [
    "## Why Finish with Fine-Tuned EfficientNetB4?\n",
    "\n",
    "To wrap up our model exploration, we return to EfficientNetB4 â€” this time training **all layers**, not just the classifier head. This full fine-tuning step makes sense as a final model because:\n",
    "\n",
    "- **Transfer learning is most powerful when adapted**: By unfreezing the base, we allow the pretrained filters to adjust to the domain-specific features of histopathology images.\n",
    "- **Best performance often comes from end-to-end optimization**: Training the entire network improves gradient flow and helps capture subtle patterns.\n",
    "- **We're building on a strong foundation**: EfficientNetB4 already performed well frozen â€” unlocking it lets us squeeze out additional performance.\n",
    "\n",
    "This model combines the **representational strength of a high-capacity pretrained CNN** with **task-specific learning**, making it a natural final step in our model progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c90e9-6c46-48de-addd-4838e208d4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92a0292-b3b2-4deb-9e10-a09f511d4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 08:52:56.169488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43710 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:56:00.0, compute capability: 8.6\n",
      "2025-08-07 08:52:56.880321: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-m8xa7mf\u001b[0m (\u001b[33mthomas-m8xa7mf-student-beans\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250807_085258-aha898mp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection/runs/aha898mp' target=\"_blank\">keras-manual-tinycnn</a></strong> to <a href='https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection' target=\"_blank\">https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection/runs/aha898mp' target=\"_blank\">https://wandb.ai/thomas-m8xa7mf-student-beans/cnn-cancer-detection/runs/aha898mp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 96, 96, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 48, 48, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48, 48, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 24, 24, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24, 24, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4718848   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4813249 (18.36 MB)\n",
      "Trainable params: 4812801 (18.36 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is deprecated and will be removed in a future release. Please use the WandbMetricsLogger, WandbModelCheckpoint, and WandbEvalCallback callbacks instead. See https://docs.wandb.ai/guides/integrations/keras for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "2025-08-07 08:53:01.578477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2025-08-07 08:53:01.721313: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-07 08:53:03.383596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7d9b387940a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-07 08:53:03.383637: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2025-08-07 08:53:03.392657: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-07 08:53:03.513178: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/2751 [..............................] - ETA: 4:41 - loss: 9.5226 - accuracy: 0.6068 - auc: 0.6251 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0269s vs `on_train_batch_end` time: 0.0605s). Check your callbacks.\n",
      "2751/2751 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.8651 - auc: 0.9307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/wandb/run-20250807_085258-aha898mp/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/wandb/run-20250807_085258-aha898mp/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/workspace/wandb/run-20250807_085258-aha898mp/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2751/2751 [==============================] - 403s 145ms/step - loss: 0.3473 - accuracy: 0.8651 - auc: 0.9307 - val_loss: 0.2884 - val_accuracy: 0.8753 - val_auc: 0.9527\n",
      "\n",
      "Epoch time: 6.71 minutes\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "IMG_SIZE   = (224, 224)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS     = 10\n",
    "SEED       = 42\n",
    "\n",
    "feature_desc = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_example(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    img = tf.image.decode_jpeg(parsed[\"image\"], channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = efficientnet_preprocess(img)\n",
    "    return img, parsed[\"label\"]\n",
    "\n",
    "def make_tfr_dataset(pattern, training):\n",
    "    ds = tf.data.Dataset.list_files(pattern, seed=SEED)\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=4,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    ds = ds.map(_parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(10_000, seed=SEED)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_tfr_dataset(\"data/train_tfr/*.tfrecord\", training=True)\n",
    "val_ds   = make_tfr_dataset(\"data/val_tfr/*.tfrecord\",   training=False)\n",
    "\n",
    "base = EfficientNetB4(\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = True  # fine-tune the entire model\n",
    "\n",
    "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "x = base(inputs)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n",
    "\n",
    "model_best = models.Model(inputs, outputs)\n",
    "model_best.compile(\n",
    "    optimizer=optimizers.Adam(1e-5),  # lower LR for fine-tuning\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "cbs = [\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, patience=2, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_best = model_best.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fdd90-0e15-43fe-af7d-c2f92ea1dd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3775dbf-de61-447f-ba48-86850f1e5957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d63eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load pretrained ResNetâ€‘18 and replace fc layer\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb7361",
   "metadata": {},
   "source": [
    "## Training loop & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76716567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement epoch loop, log loss & AUC to wandb\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8f943",
   "metadata": {},
   "source": [
    "## Experiment variants (Modelâ€‘1, Modelâ€‘2, â€¦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987db625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try deeper backbones / augmentations / samplers\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d9ada",
   "metadata": {},
   "source": [
    "## Comparison & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pull runs via wandb API and plot val AUCs\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c477a9",
   "metadata": {},
   "source": [
    "## Submission & leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acfbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create submission.csv and submit via Kaggle CLI\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e954f",
   "metadata": {},
   "source": [
    "## Conclusions & next steps\n",
    "*Key findings here.*\n",
    "\n",
    "**TODO Weekâ€‘4:** â€¦"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
