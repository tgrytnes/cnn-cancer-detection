CNN Cancer Detection (Histopathologic Cancer Detection)

Identify metastatic cancer in small image patches from digital pathology slides.
This repository contains my Week-3 mini-project for the Histopathologic Cancer Detection Kaggle challenge, structured for clarity, reproducibility, and peer review.

This project aligns with the course deliverables (notebook/report, public GitHub repo, and leaderboard screenshot) and grading rubric covering problem description, EDA, model architecture, results/analysis, conclusion, and organized deliverables.  ï¿¼

â¸»

ğŸ§­ Project Overview
	â€¢	Task: Binary image classification (cancer vs. non-cancer).
	â€¢	Objective: Build a robust CNN baseline and compare transfer-learning models, report validation metrics, and submit to Kaggle.
	â€¢	Frameworks: TensorFlow/Keras, Weights & Biases (optional).

â¸»

ğŸ“¦ Repository Structure

.
â”œâ”€ notebooks/                 # Jupyter notebooks (EDA, training, evaluation)
â”œâ”€ src/                       # Minimal helpers (datasets, training loops, utils)
â”œâ”€ submissions/               # Kaggle CSV submissions
â”œâ”€ assets/                    # Figures, leaderboard screenshot
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ LICENSE

Large datasets are not tracked in Git. Place data under data/ (ignored) or set DATA_DIR in notebooks.

â¸»

âš™ï¸ Setup
	1.	Python environment

python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

	2.	(Optional) W&B logging

export WANDB_PROJECT=cancer-detection
export WANDB_MODE=online   # or 'offline'

	3.	Kaggle data (CLI)

kaggle competitions download -c histopathologic-cancer-detection -p data/
unzip -q data/histopathologic-cancer-detection.zip -d data/histopathologic-cancer-detection


â¸»

ğŸ§ª How to Run
	â€¢	EDA: open notebooks/01_eda.ipynb
	â€¢	Training (examples):
	â€¢	notebooks/02_train_simple_cnn.ipynb
	â€¢	notebooks/03_train_mobilenetv2.ipynb
	â€¢	notebooks/04_train_efficientnetb4.ipynb
	â€¢	Inference & submission: notebooks/05_infer_and_submit.ipynb â†’ writes submissions/<timestamp>_submission.csv

Reproducibility
Key seeds used across notebooks:

SEED = 42  # tf, np, python


â¸»

ğŸ“Š Results

Validation (held-out split)

Model	Params	Val AUC	Val Acc	Notes
EfficientNet-B4 (tuned)	~17M	0.9905	0.9600	Best model; strong regularization & LR schedule
MobileNetV2	~2.3M	0.9360	â€“	Lightweight baseline
Simple-CNN baseline	<1M	â€“	â€“	Sanity check model to validate pipeline

Takeaways
	â€¢	Prefer the model with the highest AUC for threshold-independent screening.
	â€¢	Tune threshold using ROC/PR curves if sensitivity is prioritized.
	â€¢	Keep Simple-CNN to guard against data/label issues and regressions.

Kaggle
	â€¢	Competition: Histopathologic Cancer Detection
	â€¢	Screenshot: assets/leaderboard.png (top submission for this repo).
	â€¢	Note: The public leaderboard may be static if the competition is closed; a screenshot is included per the assignment.  ï¿¼

â¸»

ğŸ—ï¸ Methods
	â€¢	Data pipeline: TFRecords / tf.data with caching, prefetch, and batched resizing.
	â€¢	Preprocessing: Model-specific preprocessing (e.g., EfficientNet), basic augmentation.
	â€¢	Training:
	â€¢	Optimizers: Adam/AdamW
	â€¢	Loss: Binary Cross-Entropy
	â€¢	Metrics: AUC (primary), Accuracy (secondary)
	â€¢	Callbacks: EarlyStopping (AUC), ModelCheckpoint (val AUC), ReduceLROnPlateau
	â€¢	Hyperparameters (typical starting point):
	â€¢	IMG_SIZE=(224,224), BATCH_SIZE=128, EPOCHS=10, SEED=42

â¸»

ğŸš€ Reproduce Best Result
	1.	Train EfficientNet-B4 notebook (04_train_efficientnetb4.ipynb).
	2.	Export the best checkpoint (.h5).
	3.	Run inference notebook (05_infer_and_submit.ipynb) to create submissions/<...>.csv.
	4.	Submit to Kaggle:

kaggle competitions submit -c histopathologic-cancer-detection \
  -f submissions/<your_file>.csv -m "EffNetB4 tuned"


â¸»

ğŸ§¾ Report & Rubric Mapping
	â€¢	Problem & Data â†’ 01_eda.ipynb (brief description + sample exploration).
	â€¢	EDA â†’ Distribution plots, basic cleaning, split rationale.
	â€¢	Model Architecture â†’ Simple-CNN, MobileNetV2, EfficientNet-B4 with justification and hyperparams.
	â€¢	Results & Analysis â†’ Tables/figures, what improved performance, what didnâ€™t, troubleshooting, and tuning summary.
	â€¢	Conclusion â†’ Key learnings, limitations, and future work.
	â€¢	Deliverables â†’ Public repo (this), notebook(s)/pdf report, and leaderboard screenshot.  ï¿¼

â¸»

ğŸ§° Practical Notes
	â€¢	Compute: Training was done on GPU; adjust batch size if using CPU.
	â€¢	Class imbalance: Monitor PR curve; consider focal loss or class weights if needed.
	â€¢	Ethics: This is a learning project; clinical deployment would require rigorous validation and oversight.



â¸»

If you want, send me your exact notebook filenames and Iâ€™ll customize the â€œHow to Runâ€ and â€œReproduce Best Resultâ€ sections to match your repo.
